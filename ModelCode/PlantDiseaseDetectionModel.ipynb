# --- STEP 1: GPU Check ---
!nvidia-smi

# --- STEP 2: Install dependencies ---
!pip install tensorflow matplotlib scikit-learn pillow

# --- STEP 3: Mount Google Drive ---
from google.colab import drive
drive.mount('/content/drive')

# --- STEP 4: Dataset paths ---

TRAIN_DIR = '/content/drive/MyDrive/plant/dataset/train'
VAL_DIR   = '/content/drive/MyDrive/plant/dataset/val'

# --- STEP 5: Imports ---
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix

IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 8

# --- STEP 6: Data Generators ---

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

val_gen = ImageDataGenerator(rescale=1./255)

train_flow = train_gen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

val_flow = val_gen.flow_from_directory(
    VAL_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

NUM_CLASSES = len(train_flow.class_indices)
CLASS_NAMES = list(train_flow.class_indices.keys())
print("Classes:", CLASS_NAMES)

# --- STEP 7: Model (Transfer Learning: MobileNetV2) ---
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input

base = MobileNetV2(include_top=False, input_shape=IMG_SIZE+(3,), weights='imagenet')
base.trainable = False

inputs = layers.Input(shape=IMG_SIZE+(3,))
x = preprocess_input(inputs)
x = base(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)

model = models.Model(inputs, outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# --- STEP 8: Callbacks ---
checkpoint = ModelCheckpoint('plant_disease_model.h5', save_best_only=True, monitor='val_accuracy', verbose=1)
earlystop = EarlyStopping(patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-6)

# --- STEP 9: Train ---
history = model.fit(
    train_flow,
    epochs=EPOCHS,
    validation_data=val_flow,
    callbacks=[checkpoint, earlystop, reduce_lr]
)

# --- STEP 10: Evaluate ---
val_flow.reset()
y_true = val_flow.classes
y_pred = model.predict(val_flow)
y_pred_labels = np.argmax(y_pred, axis=1)

print(classification_report(y_true, y_pred_labels, target_names=CLASS_NAMES))

# --- STEP 11: Save class names ---
import json
with open('class_names.json','w') as f:
    json.dump(CLASS_NAMES, f)

# --- STEP 12: Download model + class names ---
from google.colab import files
files.download('plant_disease_model.h5')
files.download('class_names.json')